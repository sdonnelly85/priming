---
title: Analysis of Dative Priming Data at 54 months
author: Seamus Donnelly
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

# Opening Files
```{r, message=FALSE, warning=FALSE}
library(brms)
library(readxl)
library(writexl)
library(rmarkdown)
library(tidybayes)
library(ggplot2)
library(tidyr)
library(dplyr)
library(GGally)
library(ggpubr)
library(tidybayes)
library(patchwork)
library(sjPlot)
library(lubridate)
library(zoo)
```


```{r}
DOD_54mo <- read_xlsx("DOD_54mo_blind.xlsx") 
N_total <- nrow(DOD_54mo) # sanity check
```

Create mismatch variables. I'm also creating some character versions of many factors to make plotting a bit easier later. 

```{r}
DOD_54mo <- DOD_54mo %>% 
 mutate(
    Prime = ifelse(Prime.Type==.5, yes="DOD", no="POD"), # Prime type, character version
    Overlap = ifelse(Verb.match==.5, yes="Overlap", no="No Overlap") # overlap
    )

DOD_54mo$bias <-"DOD" # set as DOD bias
DOD_54mo$bias[DOD_54mo$Prime.verb=="passed"] = "POD" # Change POD-biased prime verbs. 
DOD_54mo$bias[DOD_54mo$Prime.verb=="sent"] = "POD"
DOD_54mo$bias[DOD_54mo$Prime.verb=="threw"] = "POD"

DOD_54mo$biasmatch <-"match" # set to match

DOD_54mo$biasmatch[DOD_54mo$bias=="POD" & DOD_54mo$Prime == "DOD"] = "mismatch" # Change to mismatch. 
DOD_54mo$biasmatch[DOD_54mo$bias=="DOD" & DOD_54mo$Prime == "POD"] = "mismatch"

table(DOD_54mo$biasmatch)

DOD_54mo$biasmismatchc <- ifelse(DOD_54mo$biasmatch == "mismatch", yes=.5, no=-.5)

```

# Missing data
There is one type of trial coded as an admin error that I don't want to remove at the time being -- trials 12r on lists 6 and 8. 
```{r}
DOD_54mo <- DOD_54mo %>%
  mutate(
    Trial_Drop = ifelse(is.na(RESPONSE.CODE), yes=1, no=0), 
    Trial_Problem = ifelse(List==6 & Item == "12r", yes = 1, no =ifelse(
      List==8 & Item == "12r", yes = 1, no =0
    ))
  )

nrow(filter(DOD_54mo, Trial_Problem==1)) == (nrow(filter(DOD_54mo, List==6))/12) + (nrow(filter(DOD_54mo, List==8))/12)

N_miss <- DOD_54mo %>%
  filter(is.na(RESPONSE.CODE)) %>%
  nrow()
```

```{r}
DOD_54mo %>%
  summarise(
    Verb_Error = sum(Verb.error, na.rm=TRUE),
    Admin_Error = sum(Admin.error, na.rm=TRUE),
    Prompts.disallowed = sum(Prompts.disallowed, na.rm=TRUE),
    Prep_Error = sum(Prep.error.bin, na.rm=TRUE), 
    Reversal_Error = sum(Reversal.error, na.rm=TRUE), 
    NonTargetResponse = sum(NonTarget.response, na.rm=TRUE), 
    No_Response = sum(No.response, na.rm=TRUE), 
    GavePOD = sum(Gave.POD, na.rm=TRUE)
     )
  

```
33 non-target responses (this includes lots of stuff, non-dative responses, preposition errors, etc). 

```{r}
DOD_54mo %>%
 filter(is.na(NonTarget.response) & is.na(RESPONSE.CODE)) %>%
  summarise(
    Verb_Error = sum(Verb.error, na.rm=TRUE),
    Admin_Error = sum(Admin.error, na.rm=TRUE),
    Prompts.disallowed = sum(Prompts.disallowed, na.rm=TRUE),
    Prep_Error = sum(Prep.error.bin, na.rm=TRUE), 
    Reversal_Error = sum(Reversal.error, na.rm=TRUE), 
    NonTargetResponse = sum(NonTarget.response, na.rm=TRUE), 
    No_Response = sum(No.response, na.rm=TRUE), 
    GavePOD = sum(Gave.POD, na.rm=TRUE)
     )

DOD_54mo %>%
  filter(is.na(RESPONSE.CODE) & is.na(NonTarget.response) & Admin.error==1)
```
10 verb errors and 5 Admin Errors. Some of those verb errors are also classified as admin errors. This is because the audio cut out on a number a few Zoom sessions and the child didn't hear the target verb and 

See how many missing values we have by participant
```{r}
DOD_54mo %>%
  group_by(Blind_ID) %>%
  mutate(
    N_Missing = sum(is.na(RESPONSE.CODE))
  ) %>%
  slice(1) %>%
  select("Blind_ID", "N_Missing") %>%
  group_by(N_Missing) %>%
  count()

DOD_54mo <- DOD_54mo %>%
  group_by(Blind_ID) %>%
  mutate(
    N_Missing = sum(is.na(RESPONSE.CODE))
  ) %>%
  slice(1) %>%
  select("Blind_ID", "N_Missing") %>%
  mutate(
    Drop = ifelse(N_Missing >= 10, 1, 0), 
    MightDrop = ifelse(N_Missing > 6, 1, 0)
  ) %>%
  full_join(DOD_54mo, .)

N_drop = DOD_54mo %>%
  filter(Drop == 1 & !is.na(RESPONSE.CODE)) %>%
  count()
  
```
One participant is missing 10 trials. 

And by item. 
```{r}
DOD_54mo %>%
  group_by(Target.verb) %>%
  mutate(
    N_Missing = sum(is.na(RESPONSE.CODE))
  ) %>%
  slice(1) %>%
  select("Target.verb", "N_Missing") %>%
  group_by(Target.verb) 
```
All non-target verbs have been removed. It doesn't make sense to estimate random effects for a level of a random factor with only a single observation. 


And let's just check the proportion of missing trials across the various experimental conditions. 
```{r}
DOD_54mo %>%
  mutate(
    Missing = ifelse(is.na(RESPONSE.CODE), yes=1, no = 0)
  ) %>%
  group_by(Prime, Overlap) %>%
  summarise(
    mean = mean(Missing), 
    sd = sd(Missing)
  )
```


```{r}
DOD_54mo %>%
  mutate(
    Missing = ifelse(is.na(RESPONSE.CODE), yes=1, no = 0)
  ) %>%
  group_by(Prime, biasmatch) %>%
  summarise(
    mean = mean(Missing), 
    sd = sd(Missing)
  )
```

The differences in missingness between condition don't look drastic. To sum up things, we have a fair amount of missing data, including 1 participants who are missing 10 or more trials. I'm goig to remove the participant who is missing 10 trials. 

```{r}
DOD_54mo <- filter(DOD_54mo, !is.na(RESPONSE.CODE) & Drop==0) # 1011

nrow(DOD_54mo) == N_total - N_miss - N_drop
```


```{r}
DOD_54mo %>%
  summarise(
    min = min(Session.Age),
    max = max(Session.Age)
  )
```

```{r}
(p1 <- DOD_54mo %>% 
  group_by(Prime, Overlap) %>%
  summarise(mean = mean(RESPONSE.CODE, na.rm=TRUE), 
            sd = sd(RESPONSE.CODE, na.rm=TRUE)) %>%
  
ggplot(aes(y=mean, x=Overlap, fill=Prime)) +
  geom_bar(stat="identity",position="dodge") +
 geom_errorbar(aes(ymin=mean, ymax=mean+sd), width=.2,
                 position=position_dodge(.9)) +
  ylim(0, .6) + 
  theme_minimal() + 
  labs(y="Raw Mean") + 
  theme(legend.position = "none")
)
```

```{r}
DOD_54mo %>% 
  group_by(Prime, Overlap, Target.verb) %>%
  summarise(mean = mean(RESPONSE.CODE, na.rm=TRUE), 
            sd = sd(RESPONSE.CODE, na.rm=TRUE)) %>%
  
ggplot(aes(y=mean, x=Overlap, fill=Prime)) +
  geom_bar(stat="identity",position="dodge") +
 geom_errorbar(aes(ymin=mean, ymax=mean+sd), width=.2,
                 position=position_dodge(.9)) +
  facet_wrap(~Target.verb) 

```
This is totally different than the other time points -- it looks like there is a lexical boost for some verbs (**showed**, **brought** and **threw**, but not others (**gave**, **passed** or **sent**).  

And let's look at the pattern of responses when we only consider participants who produced at least 1 DOD. 
```{r}
DOD_54mo <- DOD_54mo %>%
  group_by(Blind_ID) %>%
  mutate(
    Prod_DOD = sum(RESPONSE.CODE)
  ) %>%
  dplyr::select(Blind_ID, Prod_DOD) %>%
  slice(1) %>%
  ungroup() %>%
  full_join(DOD_54mo, ., by="Blind_ID")

write_xlsx(DOD_54mo, "DOD_54mo_prepped.xlsx")
```


```{r}
DOD_54mo2 <- filter(DOD_54mo, Prod_DOD > 0)

DOD_54mo2 %>%
  group_by(Blind_ID) %>%
  slice(1) %>%
  ungroup() %>%
  count()
```

```{r}
(p1b <- DOD_54mo2 %>% 
  group_by(Prime, Overlap) %>%
  summarise(mean = mean(RESPONSE.CODE, na.rm=TRUE), 
            sd = sd(RESPONSE.CODE, na.rm=TRUE)) %>%
  
ggplot(aes(y=mean, x=Overlap, fill=Prime)) +
  geom_bar(stat="identity",position="dodge") +
 geom_errorbar(aes(ymin=mean, ymax=mean+sd), width=.2,
                 position=position_dodge(.9)) +
  ylim(0, .9) + 
  theme_minimal() + 
  labs(y="Raw Mean") + 
  theme(legend.position = "none")
)
```


```{r}
DOD_54mo2 %>% 
  group_by(Prime, Overlap, Target.verb) %>%
  summarise(mean = mean(RESPONSE.CODE, na.rm=TRUE), 
            sd = sd(RESPONSE.CODE, na.rm=TRUE)) %>%
  
ggplot(aes(y=mean, x=Overlap, fill=Prime)) +
  geom_bar(stat="identity",position="dodge") +
 geom_errorbar(aes(ymin=mean, ymax=mean+sd), width=.2,
                 position=position_dodge(.9)) +
  facet_wrap(~Target.verb) 

```
 

Plot proprortion of DOD responses by each cell in two-way interaction between biasmatch and prime type. 
```{r}
(p1c <- DOD_54mo %>% 
  filter(Overlap == "No Overlap") %>%
  group_by(Prime, biasmatch) %>%
  summarise(mean = mean(RESPONSE.CODE, na.rm=TRUE), 
            sd = sd(RESPONSE.CODE, na.rm=TRUE)) %>%
  
ggplot(aes(y=mean, x=biasmatch, fill=Prime)) +
  geom_bar(stat="identity",position="dodge") +
 geom_errorbar(aes(ymin=mean, ymax=mean+sd), width=.2,
                 position=position_dodge(.75)) +
  ylim(0, .9) + 
  theme_minimal() + 
  labs(y="Raw Mean")
)
```

```{r}
DOD_54mo %>% 
  filter(Overlap == "No Overlap") %>%
  group_by(Prime, biasmatch, Target.verb) %>%
  summarise(mean = mean(RESPONSE.CODE, na.rm=TRUE), 
            sd = sd(RESPONSE.CODE, na.rm=TRUE)) %>%
  
ggplot(aes(y=mean, x=biasmatch, fill=Prime)) +
  geom_bar(stat="identity",position="dodge") +
 geom_errorbar(aes(ymin=mean, ymax=mean+sd), width=.2,
                 position=position_dodge(.9)) +
  facet_wrap(~Target.verb) 
```
```{r}
DOD_54mo %>% 
  filter(Overlap == "No Overlap") %>%
  group_by(Target.verb, Prime, biasmatch) %>%
  count()
```
Again, it looks like the pattern is different on the levels of aggregate data and indiviaul verbs, which was the case with the 42 month verbs. 

```{r}
DOD_54mo %>%
   filter(Overlap == "No Overlap") %>% 
   filter(Target.verb=="gave" & Prime=="DOD" & biasmatch == "match")

DOD_54mo %>%
   filter(Overlap == "No Overlap") %>% 
   filter(Target.verb=="gave" & Prime=="POD" & biasmatch == "mismatch")
```

# Lexical Boost

## All Participants
```{r}
logit_prior = prior(normal(0,2), class=sd)
```

First lets consider whether we should include random effects by the interaction between ID and Item. In principle this is possible, but I think it will greatly complicated the model. So I'll fit two variance component models with the two random effects specifications and compare them to one another. 
```{r}
m0<- brm(RESPONSE.CODE ~ 1 + (1 | Blind_ID) + (1 | Target.verb),  family="bernoulli", prior=logit_prior, iter=4000, cores=4, data=DOD_54mo, file="54mo/output/m0.rds")
```

```{r}
m0b <- brm(RESPONSE.CODE ~ 1 + (1 | Blind_ID) + (1 | Target.verb) + (1 | Blind_ID:Target.verb),  family="bernoulli", prior=logit_prior, iter=4000, cores=4, data=DOD_54mo, file="54mo/output/m0b.rds")
```

Add loo for loo_compare
```{r}
m0<- add_criterion(m0, "loo")
m0b<- add_criterion(m0b, "loo")
```

```{r}
loo(m0, m0b, reloo=TRUE)
model_weights(m0, m0b)
```
Simpler model fits numerically better. 


Here's the model of the lexical boost. 
```{r}
m1 <- brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=DOD_54mo, file="54mo/output/m1.rds")

add_criterion(m1, "loo")
```

```{r}
summary(m1)
hypothesis(m1, "Prime.Type > 0")
hypothesis(m1, "Prime.Type:OverlapOverlap > 0")
```

A quick check that the posterior predictive distribution looks okay. 
```{r}
pp_check(m1)
```
A quick check that the chains look okay. 
```{r}
#plot(m1)
```


```{r}
p2 <- fitted(m1, re_formula=NA, scale="response") %>%
  cbind(DOD_54mo) %>%
  dplyr::select(Prime, Overlap, Estimate, Est.Error) %>%
  ggplot(aes(y=Estimate, x=Overlap, fill=Prime )) +
  geom_bar(stat="identity",position="dodge") + 
  geom_errorbar(aes(ymin=Estimate, ymax=Estimate+Est.Error), width=.2,
                 position=position_dodge(.9)) + 
  theme_minimal() + 
  ylim(0, .6) + 
  labs(y="Predicted Probability") + 
  theme(legend.position = "none")
 

p1 | p2 + plot_layout(guides = "collect") & theme(legend.position = "right") 
```

```{r}
library("tidybayes")
ps_means1 <- posterior_samples(m1) %>%
  dplyr::select(Intc = "b_Intercept", Prime = "b_Prime.Type", Overlap= "b_OverlapOverlap", Int = "b_Prime.Type:OverlapOverlap") %>%
  mutate(
    DOD_no = inv_logit_scaled(Intc + .5*Prime),
    POD_no = inv_logit_scaled(Intc  - .5*Prime),
    RR_ap = DOD_no/POD_no,
    DOD_o = inv_logit_scaled(Intc + .5*Prime + Overlap + .5*Int),
    POD_o = inv_logit_scaled(Intc - .5*Prime + Overlap - .5*Int),
    RR_ov = (DOD_o/POD_o)
  ) %>%
  dplyr::select(RR_ap, RR_ov) %>%
  pivot_longer(everything(), names_to = "RR", values_to = "value") %>%
  mutate(
    Effect = ifelse(RR=="RR_ap", yes= "Abstract Priming", no="Lexically Boosted Priming")
  ) 

ps_means1 %>%
  group_by(Effect) %>%
  dplyr::select(-"RR") %>%
  median_qih(.width=.9)

RR1 <- ggplot(ps_means1, aes(x=value, fill=Effect)) + geom_density(alpha = .2) + xlim(0, 15) + geom_vline(xintercept=1) + theme_minimal() + xlab("RR")

RR1
```

Consider only non-Zoom session kids, in case there's a difference. 
```{r}
m1b <- DOD_54mo %>%
  filter(Trial_Problem == 0) %>%
  brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m1b.rds")
summary(m1b)
hypothesis(m1b, "Prime.Type > 0")
hypothesis(m1b, "Prime.Type:OverlapOverlap > 0")

m1c <- DOD_54mo %>%
  filter(Zoom==1) %>%
brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m1c.rds")
summary(m1c)
hypothesis(m1c, "Prime.Type > 0")
hypothesis(m1c, "Prime.Type:OverlapOverlap > 0")

m1d <-  DOD_54mo %>%
  mutate(
    k = m1$criteria$loo$diagnostics$pareto_k
  ) %>%
  filter(k < .7) %>%
   brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m1d.rds")
summary(m1d)
hypothesis(m1d, "Prime.Type > 0")
hypothesis(m1d, "Prime.Type:OverlapOverlap > 0")
```
## Grammatical Only

```{r}
m2 <- brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=DOD_54mo2, file="54mo/output/m2.rds")
m2 <- add_criterion(m2, "loo")

summary(m2)
hypothesis(m2, "Prime.Type > 0")
hypothesis(m2, "Prime.Type:OverlapOverlap > 0")
```


```{r}
p2 <- fitted(m2, re_formula=NA, scale="response") %>%
  cbind(DOD_54mo2) %>%
  dplyr::select(Prime, Overlap, Estimate, Est.Error) %>%
  ggplot(aes(y=Estimate, x=Overlap, fill=Prime )) +
  geom_bar(stat="identity",position="dodge") + 
  geom_errorbar(aes(ymin=Estimate, ymax=Estimate+Est.Error), width=.2,
                 position=position_dodge(.9)) + 
  theme_minimal() + 
  ylim(0, .9) + 
  labs(y="Predicted Probability") + 
  theme(legend.position = "none")
 

p1b | p2 + plot_layout(guides = "collect") & theme(legend.position = "right") 
```


```{r}
ps_means2 <- posterior_samples(m2) %>%
  dplyr::select(Intc = "b_Intercept", Prime = "b_Prime.Type", Overlap= "b_OverlapOverlap", Int = "b_Prime.Type:OverlapOverlap") %>%
  mutate(
    DOD_no = inv_logit_scaled(Intc + .5*Prime),
    POD_no = inv_logit_scaled(Intc  - .5*Prime),
    RR_ap = DOD_no/POD_no,
    DOD_o = inv_logit_scaled(Intc + .5*Prime + Overlap + .5*Int),
    POD_o = inv_logit_scaled(Intc - .5*Prime + Overlap - .5*Int),
    RR_ov = (DOD_o/POD_o)
  ) %>%
  dplyr::select(RR_ap, RR_ov) %>%
  pivot_longer(everything(), names_to = "RR", values_to = "value") %>%
  mutate(
    Effect = ifelse(RR=="RR_ap", yes= "Abstract Priming", no="Lexically Boosted Priming")
  ) 

ps_means2 %>%
  group_by(Effect) %>%
  dplyr::select(-"RR") %>%
  median_qih(width=.90)

RR2 <- ggplot(ps_means2, aes(x=value, fill=Effect)) + geom_density(alpha = .2) + xlim(0, 15) + geom_vline(xintercept=1) + theme_minimal() + xlab("RR")

RR2
```

```{r}
m2b <- DOD_54mo2 %>%
  filter(Trial_Problem == 0) %>%
  brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m2b.rds")
summary(m2b)
hypothesis(m2b, "Prime.Type > 0")
hypothesis(m2b, "Prime.Type:OverlapOverlap > 0")

m2c <- DOD_54mo2 %>%
  filter(Zoom==1) %>%
brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m2c.rds")
summary(m2c)
hypothesis(m2c, "Prime.Type > 0")
hypothesis(m2c, "Prime.Type:OverlapOverlap > 0")

m2d <-  DOD_54mo2 %>%
  mutate(
    k = m2$criteria$loo$diagnostics$pareto_k
  ) %>%
  filter(k < .7) %>%
   brm(RESPONSE.CODE ~ 1 + Prime.Type*Overlap  +  (1 +  Prime.Type*Overlap | Blind_ID) + (1 +  Prime.Type*Overlap   | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m2d.rds")
summary(m2d)
hypothesis(m2d, "Prime.Type > 0")
hypothesis(m2d, "Prime.Type:OverlapOverlap > 0")
```


```{r}
tab_model(m1, m2, show.re.var=FALSE, show.r2=FALSE, show.icc=FALSE, transform=NULL, collapse.ci=TRUE, p.style="scientific", ci.hyphen = " : ", dv.labels=c("Model 1",  "Model 2"), string.stat="",
          pred.labels=c(
            "Intercept", 
            "Prime (DOD)", 
            "Overlap", 
            "Prime * Overlap"
          ))
```

# Bias Mismatch
```{r}
DOD_54mo3 <- filter(DOD_54mo, Overlap=="No Overlap")
m3 <- brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 + Prime.Type*biasmatch| Blind_ID) + (1 + Prime.Type*biasmatch| Target.verb), logit_prior, family="bernoulli", iter=4000, cores=4, file="54mo/output/m3.rds", data=DOD_54mo3)
summary(m3)
m3 <- add_criterion(m3, "loo")
```

```{r}
plot(m3)
pp_check(m3)
```

```{r}
hypothesis(m3, "Prime.Type > 0")
hypothesis(m3, "Prime.Type:biasmatchmismatch > 0") # Bias mismatch
```

```{r}
m3b <- DOD_54mo3 %>%
  filter(Trial_Problem == 0) %>%
  brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 +Prime.Type*biasmatch  | Blind_ID) + (1 +Prime.Type*biasmatch | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m3b.rds")
summary(m3b)
hypothesis(m3b, "Prime.Type > 0")
hypothesis(m3b, "Prime.Type:biasmatchmismatch > 0")

m3c <- DOD_54mo3  %>%
  filter(Zoom==1) %>%
brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 +Prime.Type*biasmatch  | Blind_ID) + (1 +Prime.Type*biasmatch | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m3c.rds")
summary(m3c)
hypothesis(m3c, "Prime.Type > 0")
hypothesis(m3c, "Prime.Type:biasmatchmismatch > 0")

m3d <- DOD_54mo3 %>%
  mutate(
    k = m3$criteria$loo$diagnostics$pareto_k
  ) %>%
  filter(k < .7) %>%
 brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 +Prime.Type*biasmatch  | Blind_ID) + (1 +Prime.Type*biasmatch | Target.verb), logit_prior, family="bernoulli", iter=4000, cores=4, file="54mo/output/m3d.rds", data=.)
summary(m3d)
hypothesis(m3d, "Prime.Type > 0")
hypothesis(m3d, "Prime.Type:biasmatchmismatch > 0")
```

```{r}
p3 <- fitted(m3, re_formula=NA, scale="response") %>%
  cbind(DOD_54mo3) %>%
  dplyr::select(Prime, biasmatch, Estimate, Est.Error) %>%
  ggplot(aes(y=Estimate, x=biasmatch, fill=Prime )) +
  geom_bar(stat="identity",position="dodge") + 
  geom_errorbar(aes(ymin=Estimate, ymax=Estimate+Est.Error), width=.2,
                 position=position_dodge(.9)) + 
  theme_minimal() + 
  ylim(0, .6) + 
  labs(y="Predicted Probability") + 
  theme(legend.position = "none")
 

p1c | p3 + plot_layout(guides = "collect") & theme(legend.position = "right") 
```

```{r}
DOD_54mo4 <-
  DOD_54mo3 %>%
  filter(Prod_DOD > 0)
```

```{r}
m4 <- brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 + Prime.Type*biasmatch| Blind_ID) + (1 + Prime.Type*biasmatch| Target.verb), logit_prior, family="bernoulli", iter=4000, cores=4, file="54mo/output/m4.rds", data=DOD_54mo4)
summary(m4)
m4 <- add_criterion(m4, "loo")
hypothesis(m4, "Prime.Type > 0")
hypothesis(m4, "Prime.Type:biasmatchmismatch > 0") # Bias mismatch
```



```{r}
m4b <- DOD_54mo4 %>%
  filter(Trial_Problem == 0) %>%
  brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 +Prime.Type*biasmatch  | Blind_ID) + (1 +Prime.Type*biasmatch | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m4b.rds")
summary(m4b)
hypothesis(m4b, "Prime.Type > 0")
hypothesis(m4b, "Prime.Type:biasmatchmismatch > 0")

m4c <- DOD_54mo3  %>%
  filter(Zoom==1) %>%
brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 +Prime.Type*biasmatch  | Blind_ID) + (1 +Prime.Type*biasmatch | Target.verb), logit_prior,family="bernoulli", cores=4, data=., file="54mo/output/m4c.rds")
summary(m4c)
hypothesis(m4c, "Prime.Type > 0")
hypothesis(m4c, "Prime.Type:biasmatchmismatch > 0")

m4d <- DOD_54mo4 %>%
  mutate(
    k = m4$criteria$loo$diagnostics$pareto_k
  ) %>%
  filter(k < .7) %>%
 brm(RESPONSE.CODE ~ 1 +Prime.Type*biasmatch + (1 +Prime.Type*biasmatch  | Blind_ID) + (1 +Prime.Type*biasmatch | Target.verb), logit_prior, family="bernoulli", iter=4000, cores=4, file="54mo/output/m4d.rds", data=.)
summary(m4d)
hypothesis(m4d, "Prime.Type > 0")
hypothesis(m4d, "Prime.Type:biasmatchmismatch > 0")
```

```{r}
tab_model(m1, m2, m3, m4, show.re.var=FALSE, show.r2=FALSE, show.icc=FALSE, transform=NULL, collapse.ci=TRUE, p.style="scientific", ci.hyphen = " : ", dv.labels=c("Model 1",  "Model 2", "Model 3", "Model 4"), string.stat="", file="Table4",
           pred.labels=c(
            "Intercept", 
            "Prime (DOD)", 
            "Overlap", 
            "Prime * Overlap",
            "Bias Mismatch (POD)", 
            "Prime * Bias Mismatch "
          ))
```

```{r}
sessionInfo()
```

